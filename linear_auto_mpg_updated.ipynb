{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to our Linear Regression Workshop!\n",
    "\n",
    "In this notebook, we will explore **Linear Regression** using a real-world dataset: **Auto MPG**.\n",
    "\n",
    "We will:\n",
    "- Understand the concept of linear regression\n",
    "- Implement linear regression from scratch\n",
    "- Apply it to the **Auto MPG dataset** to predict a car's fuel efficiency (**miles per gallon - MPG**)\n",
    "- Evaluate the performance of our model using appropriate metrics\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us understand what \"regression\" and \"linear regression\" mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "- Regression is a statistical technique that relates a dependent variable (denoted as Y) to one or more independent variables (denoted as X).\n",
    "\n",
    "- Regression models are able to show whether the changes observed in the dependent variable are associated with changes in the independent variables.\n",
    "\n",
    "- There are many different regression models, such as \"linear regression\", \"logistic regression\", \"polynomial regression\", \"ridge regression\", and \"lasso regression\", and many others.\n",
    "\n",
    "- In this notebook, we will focus on linear regression, which models the relationship between one dependent variable and one or more independent variables using a straight line, hence the name \"linear\" regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "- Linear regression functions are of the form `Y = wX + b`, where\n",
    "\n",
    "  `Y`: dependent variable vector, what we are trying to guess\n",
    "\n",
    "  `w`: weight vector, the importance of each independent variable. We will refer to weights as `coefficients` in this workshop, to make the code more understandable.\n",
    "\n",
    "  `X`: independent variable vector, what we base our predictions on\n",
    "\n",
    "  `b`: bias, helps with fitting the model better. We will refer to bias as `intercept` in this workshop, again to make the code more understandable.\n",
    "\n",
    "- The purpose of `finding` the most accurate model requires us to find the best parameters `w` and `b` such that our regression line is a perfect fit for our dataset. That is, we try many many many different `w` and `b` values to find the line that helps us the most. But how do we find them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "- MSE is a very common measure of the quality of an estimator. It's used to evaluate the accuracy of a linear regression model by calculating the average of the squares of the errors (difference between actual and predicted results).\n",
    "\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n",
    "$$\n",
    "\n",
    "- The end goal is to minimize MSE to improve the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Gradient Descent is an optimization algorithm used to minimize the cost function (MSE in our case), by iteratively adjusting model parameters. The update rule for each parameter $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "- $\\theta$ : the parameter we are trying to optimize (w or b for linear regression)\n",
    "\n",
    "- $\\alpha$ : learning rate, a hyperparameter that determines the step size at each iteration while moving toward a minimum cost function. Small alpha ensures more precise updates, but training will take more time with smaller alpha (since changes will be very minimal, but very accurate).\n",
    "\n",
    "- $J(\\theta)$ : the cost function, MSE in our case.\n",
    "\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ : Gradient of the cost function with respect to the parameter. It measures how much J changes with a small change in $\\theta$. The gradient points in the direction of the steepest increase of the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we know what linear regression looks like, let us start implementing it using Python!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install joblib matplotlib numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the Linear Regression class using Python together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class LinearRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            method=\"gradient_descent\",\n",
    "            learning_rate=0.01,\n",
    "            epochs=1000,\n",
    "            regularization=None,\n",
    "            alpha=0.01,\n",
    "            momentum=0.9\n",
    "    ):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha  # Regularization strength\n",
    "        self.momentum = momentum  # Momentum factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)  # Ensure y is a column vector\n",
    "\n",
    "        if self.method == \"normal_equation\":\n",
    "            self._fit_normal_equation(X, y)\n",
    "        elif self.method == \"gradient_descent\":\n",
    "            self._fit_gradient_descent(X, y)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'normal_equation' or 'gradient_descent'\")\n",
    "\n",
    "    def _fit_normal_equation(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n",
    "        if self.regularization == \"ridge\":\n",
    "            L = self.alpha * np.eye(X_b.shape[1])\n",
    "            L[0, 0] = 0  # Do not regularize the intercept term\n",
    "            theta_best = np.linalg.inv(X_b.T.dot(X_b) + L).dot(X_b.T).dot(y)\n",
    "        else:\n",
    "            theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        self.intercept = theta_best[0, 0]\n",
    "        self.coefficients = theta_best[1:].flatten()\n",
    "\n",
    "    def _fit_gradient_descent(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n",
    "        m = len(y)\n",
    "        theta = np.random.randn(X_b.shape[1], 1)\n",
    "        velocity = np.zeros_like(theta)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "            if self.regularization == \"ridge\":\n",
    "                regularization_term = self.alpha * np.r_[np.zeros((1, 1)), theta[1:]]\n",
    "                gradients += 2 * regularization_term\n",
    "\n",
    "            velocity = self.momentum * velocity - self.learning_rate * gradients\n",
    "            theta += velocity\n",
    "\n",
    "        self.intercept = theta[0, 0]\n",
    "        self.coefficients = theta[1:].flatten()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(np.r_[self.intercept, self.coefficients])\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Scoring function required for hyperparameter tuning (uses R2 score).\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return self.r2_score(y, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def r2_score(y_true, y_pred):\n",
    "        total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        explained_variance = np.sum((y_true - y_pred) ** 2)\n",
    "        return 1 - explained_variance / total_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class definition for Linear Regression covers everything we have discussed in theory, and further implements other metrics for accuracy calculations and optimizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "We will use the **Auto MPG dataset**, which contains information about different car models from the 1970s and 1980s.\n",
    "Our goal is to predict **MPG (miles per gallon)** based on various car attributes.\n",
    "\n",
    "### Features:\n",
    "- `horsepower`: Engine power of the car\n",
    "- `weight`: Weight of the vehicle\n",
    "- `acceleration`: Acceleration capability\n",
    "- `cylinders`: Number of cylinders in the engine\n",
    "- `displacement`: Engine displacement\n",
    "\n",
    "### Target Variable:\n",
    "- `mpg`: Miles per gallon (fuel efficiency), which we are trying to predict.\n",
    "\n",
    "### Why this dataset?\n",
    "- It provides a **clear linear relationship** between features and the target variable.\n",
    "- It's a **real-world problem** that showcases how linear regression can be used in automotive efficiency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Auto MPG dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv\"\n",
    "auto_mpg = pd.read_csv(url)\n",
    "\n",
    "# Drop missing values\n",
    "auto_mpg = auto_mpg.dropna()\n",
    "\n",
    "# Select features and target\n",
    "X = auto_mpg[[\"horsepower\", \"weight\", \"acceleration\", \"cylinders\", \"displacement\", \"model_year\"]]\n",
    "y = auto_mpg[\"mpg\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Display dataset info\n",
    "print(auto_mpg.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auto_mpg.info())  # see the official description of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the data to understand it better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_mpg.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histograms of the different features, we can conclude that:\n",
    "\n",
    "1. Features are distributed on very different scales\n",
    "\n",
    "For better accuracy, we should preprocess those features. We can either perform feature engineering or clean those problematic instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the relationship between the target variable `mpg` and the features to understand the data better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get rid of the non-numeric columns, origin and name because they are not useful for our regression model.\n",
    "auto_mpg = auto_mpg.drop(columns=[\"origin\", \"name\"])\n",
    "\n",
    "features = [\"cylinders\", \"horsepower\", \"weight\", \"acceleration\", \"displacement\", \"model_year\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 2 columns\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    row, col = divmod(i, 3)\n",
    "    axes[row, col].scatter(auto_mpg[feature], auto_mpg[\"mpg\"], alpha=0.6)\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel(\"MPG (Fuel Efficiency)\")\n",
    "    axes[row, col].set_title(f\"{feature} vs. MPG\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the correlations of other variables with MPG. The values range from -1 to 1. A value of 1 means a perfect positive correlation, and a value of -1 means a perfect negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = auto_mpg.corr()\n",
    "corr[\"mpg\"].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `weight`, `displacement`, and `horsepower` have a strong negative correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun part, generate an instance of our Linear Regression model and train it on the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1],  # Try different learning rates\n",
    "    \"epochs\": [1000, 2000, 5000],  # Try different epochs\n",
    "    \"regularization\": [None, \"ridge\"],  # Try Ridge regularization\n",
    "    \"alpha\": [0.001, 0.01, 0.1, 1],  # Ridge alpha values\n",
    "    \"momentum\": [0.5, 0.9, 0.99],  # Different momentum values\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(LinearRegression(), param_grid, cv=3, scoring='r2', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train the best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(\n",
    "    method=\"gradient_descent\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    regularization=best_params[\"regularization\"],\n",
    "    alpha=best_params[\"alpha\"],\n",
    "    momentum=best_params[\"momentum\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, let's evaluate its performance on the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "mse = model.mean_squared_error(y_test, predictions)\n",
    "r2 = model.r2_score(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~80% accuracy is not bad for a simple linear regression model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our regression line and the actual data points on the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Select two strong predictors based on heatmap\n",
    "X = auto_mpg[[\"horsepower\", \"weight\"]].values\n",
    "y = auto_mpg[\"mpg\"].values\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate predictions\n",
    "x_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)\n",
    "y_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)\n",
    "x_mesh, y_mesh = np.meshgrid(x_range, y_range)\n",
    "z_pred = model.predict(np.c_[x_mesh.ravel(), y_mesh.ravel()]).reshape(x_mesh.shape)\n",
    "\n",
    "# Plot 3D graph\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Scatter actual data points\n",
    "ax.scatter(X[:, 0], X[:, 1], y, color=\"blue\", alpha=0.5, label=\"Actual Data\")\n",
    "\n",
    "# Plot regression surface\n",
    "ax.plot_surface(x_mesh, y_mesh, z_pred, color=\"red\", alpha=0.3)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"Horsepower\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.set_zlabel(\"MPG\")\n",
    "ax.set_title(\"3D Regression: Horsepower & Weight vs. MPG\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our model by using more advanced regression techniques, such as polynomial regression, ridge regression, or lasso regression. We can also try to preprocess our data more effectively to remove outliers and scale our features. In the next workshop, we will discuss these techniques and how to implement them using Python, and see if we can achieve more accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
