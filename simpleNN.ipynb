{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5143b0",
   "metadata": {},
   "source": [
    "# Basic Neural Networks\n",
    "\n",
    "Today, we will implement a very simple neural network from scratch. We will use the sigmoid activation function and the mean squared error loss function. We will train the network using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a4312",
   "metadata": {},
   "source": [
    "Let's first implement several loss functions and their derivatives, since we will need them later for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada8abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd4590",
   "metadata": {},
   "source": [
    "# YOUR TURN: Implement the following activation functions, as seen from the lecture notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df8b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Sigmoid activation function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    # Tanh activation function\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # ReLU activation function\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax activation function\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6464d8",
   "metadata": {},
   "source": [
    "Since we will use the sigmoid activation function, we will need its derivative. We will also need the mean squared error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458ce8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    # Derivative of sigmoid activation function\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def MSE(y, y_pred):\n",
    "    # Mean Squared Error\n",
    "    return np.mean((y - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45c3e8",
   "metadata": {},
   "source": [
    "Now let's define the neural network. We will make the layer sizes and the learning rate configurable parameters. We will also initialize the weights and biases randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e1aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, lr=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with:\n",
    "        - layer_sizes: list defining the number of neurons in each layer\n",
    "        - lr: learning rate for weight updates\n",
    "        \"\"\"\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.layer_sizes = layer_sizes  # Structure of the neural network\n",
    "\n",
    "        # Initialize weights and biases with random values\n",
    "        self.weights = [\n",
    "            np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            np.random.randn(layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: propagates inputs through the network layer by layer\n",
    "        - Stores activations for use in backpropagation\n",
    "        - Uses sigmoid activation function\n",
    "        \"\"\"\n",
    "        self.activations = [x]  # Store activations for backpropagation\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            x = np.dot(x, W) + b  # Linear transformation\n",
    "            x = sigmoid(x)  # Apply activation function\n",
    "            self.activations.append(x)  # Store activation\n",
    "        return x  # Final output\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Backward pass: computes gradients and updates weights using gradient descent\n",
    "        \"\"\"\n",
    "        # Compute gradient of loss w.r.t. output (last layer)\n",
    "        loss_gradient = (self.activations[-1] - y) * sigmoid_derivative(\n",
    "            self.activations[-1]\n",
    "        )\n",
    "        gradients = [loss_gradient]\n",
    "\n",
    "        # Backpropagate the error through hidden layers\n",
    "        for i in range(len(self.weights) - 1, 0, -1):\n",
    "            loss_gradient = np.dot(\n",
    "                gradients[-1], self.weights[i].T\n",
    "            ) * sigmoid_derivative(self.activations[i])\n",
    "            gradients.append(loss_gradient)\n",
    "\n",
    "        gradients.reverse()  # Reverse to align with layer ordering\n",
    "\n",
    "        # Update weights and biases using the computed gradients\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * np.dot(self.activations[i].T, gradients[i])\n",
    "            self.biases[i] -= self.lr * np.sum(gradients[i], axis=0)\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        \"\"\"\n",
    "        Train the neural network using forward and backward propagation\n",
    "        - X: input features\n",
    "        - y: target labels\n",
    "        - epochs: number of training iterations\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)  # Forward pass\n",
    "            self.backward(X, y)  # Backward pass and weight updates\n",
    "\n",
    "            # Print loss every 100 epochs for monitoring\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {MSE(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca24ce",
   "metadata": {},
   "source": [
    "Now let's initialize the network and train it on a real life dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448ed2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b15dd",
   "metadata": {},
   "source": [
    "We will use the breast cancer dataset from sklearn. This dataset is a binary classification dataset, which makes it a good candidate for our simple neural network.\n",
    "Breast cancer wisconsin (diagnostic) dataset is a classic and very easy binary classification dataset. It contains 569 samples of malignant and benign tumor cells. The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant. Let's do a quick exploration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021a542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e39996",
   "metadata": {},
   "source": [
    "Now let's prepare the dataset and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48103252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4032502278620832\n",
      "Epoch 100, Loss: 0.011348672389084706\n",
      "Epoch 200, Loss: 0.008183841185131893\n",
      "Epoch 300, Loss: 0.006416120652152135\n",
      "Epoch 400, Loss: 0.005115304928408777\n",
      "Epoch 500, Loss: 0.004222833230006237\n",
      "Epoch 600, Loss: 0.003692371236565765\n",
      "Epoch 700, Loss: 0.003350637304201768\n",
      "Epoch 800, Loss: 0.003117035339809622\n",
      "Epoch 900, Loss: 0.002950445502803346\n",
      "Epoch 1000, Loss: 0.0028276126989517977\n",
      "Epoch 1100, Loss: 0.0027344730740917773\n",
      "Epoch 1200, Loss: 0.002662110574139502\n",
      "Epoch 1300, Loss: 0.0026046692935225344\n",
      "Epoch 1400, Loss: 0.0025581922094030296\n",
      "Epoch 1500, Loss: 0.0025199381079503316\n",
      "Epoch 1600, Loss: 0.002487963467270327\n",
      "Epoch 1700, Loss: 0.0024608589513584068\n",
      "Epoch 1800, Loss: 0.0024375791661777903\n",
      "Epoch 1900, Loss: 0.0024173297814073656\n",
      "Epoch 2000, Loss: 0.002399490172620031\n",
      "Epoch 2100, Loss: 0.00238355762221964\n",
      "Epoch 2200, Loss: 0.002369103224616727\n",
      "Epoch 2300, Loss: 0.002355730823266033\n",
      "Epoch 2400, Loss: 0.0023430278901257614\n",
      "Epoch 2500, Loss: 0.0023304874168288093\n",
      "Epoch 2600, Loss: 0.0023173487596145096\n",
      "Epoch 2700, Loss: 0.0023021985596986213\n",
      "Epoch 2800, Loss: 0.002281730007887184\n",
      "Epoch 2900, Loss: 0.0022455984315642593\n",
      "Epoch 3000, Loss: 0.0021428615184310128\n",
      "Epoch 3100, Loss: 0.0016220455275640722\n",
      "Epoch 3200, Loss: 0.0009521748728506706\n",
      "Epoch 3300, Loss: 0.0006489610694517703\n",
      "Epoch 3400, Loss: 0.0004983798717812432\n",
      "Epoch 3500, Loss: 0.0004079969037653031\n",
      "Epoch 3600, Loss: 0.0003460326527683506\n",
      "Epoch 3700, Loss: 0.00030051374340126354\n",
      "Epoch 3800, Loss: 0.00026567939879871747\n",
      "Epoch 3900, Loss: 0.00023822082093949987\n",
      "Epoch 4000, Loss: 0.00021604450197342136\n",
      "Epoch 4100, Loss: 0.00019775895970273354\n",
      "Epoch 4200, Loss: 0.00018241102390454615\n",
      "Epoch 4300, Loss: 0.00016933158958240565\n",
      "Epoch 4400, Loss: 0.00015804004025981455\n",
      "Epoch 4500, Loss: 0.00014818345276405335\n",
      "Epoch 4600, Loss: 0.00013949722419372195\n",
      "Epoch 4700, Loss: 0.00013177911988532183\n",
      "Epoch 4800, Loss: 0.0001248718456631401\n",
      "Epoch 4900, Loss: 0.0001186511166978123\n",
      "Epoch 5000, Loss: 0.0001130173309361134\n",
      "Epoch 5100, Loss: 0.00010788964898508548\n",
      "Epoch 5200, Loss: 0.00010320170998132073\n",
      "Epoch 5300, Loss: 9.889847966711806e-05\n",
      "Epoch 5400, Loss: 9.493389559067308e-05\n",
      "Epoch 5500, Loss: 9.126908273057978e-05\n",
      "Epoch 5600, Loss: 8.78709836128532e-05\n",
      "Epoch 5700, Loss: 8.471129395068314e-05\n",
      "Epoch 5800, Loss: 8.176562650047174e-05\n",
      "Epoch 5900, Loss: 7.901284750654915e-05\n",
      "Epoch 6000, Loss: 7.64345451694466e-05\n",
      "Epoch 6100, Loss: 7.401460018518536e-05\n",
      "Epoch 6200, Loss: 7.173883597974352e-05\n",
      "Epoch 6300, Loss: 6.959473173993373e-05\n",
      "Epoch 6400, Loss: 6.757118534785701e-05\n",
      "Epoch 6500, Loss: 6.565831628848278e-05\n",
      "Epoch 6600, Loss: 6.384730081336781e-05\n",
      "Epoch 6700, Loss: 6.213023331355924e-05\n",
      "Epoch 6800, Loss: 6.050000912628368e-05\n",
      "Epoch 6900, Loss: 5.895022497655834e-05\n",
      "Epoch 7000, Loss: 5.747509401087981e-05\n",
      "Epoch 7100, Loss: 5.6069372969929085e-05\n",
      "Epoch 7200, Loss: 5.472829951064269e-05\n",
      "Epoch 7300, Loss: 5.344753805457401e-05\n",
      "Epoch 7400, Loss: 5.222313283133e-05\n",
      "Epoch 7500, Loss: 5.1051467019620895e-05\n",
      "Epoch 7600, Loss: 4.992922707677711e-05\n",
      "Epoch 7700, Loss: 4.885337150010953e-05\n",
      "Epoch 7800, Loss: 4.7821103387662914e-05\n",
      "Epoch 7900, Loss: 4.6829846267508654e-05\n",
      "Epoch 8000, Loss: 4.587722274822449e-05\n",
      "Epoch 8100, Loss: 4.4961035612153904e-05\n",
      "Epoch 8200, Loss: 4.407925103020079e-05\n",
      "Epoch 8300, Loss: 4.322998362450116e-05\n",
      "Epoch 8400, Loss: 4.24114831450799e-05\n",
      "Epoch 8500, Loss: 4.1622122559959186e-05\n",
      "Epoch 8600, Loss: 4.086038738626423e-05\n",
      "Epoch 8700, Loss: 4.0124866113586915e-05\n",
      "Epoch 8800, Loss: 3.941424159096237e-05\n",
      "Epoch 8900, Loss: 3.872728326589987e-05\n",
      "Epoch 9000, Loss: 3.8062840178470615e-05\n",
      "Epoch 9100, Loss: 3.741983462590739e-05\n",
      "Epoch 9200, Loss: 3.679725642385106e-05\n",
      "Epoch 9300, Loss: 3.619415769955703e-05\n",
      "Epoch 9400, Loss: 3.560964816028605e-05\n",
      "Epoch 9500, Loss: 3.504289078694534e-05\n",
      "Epoch 9600, Loss: 3.449309790896311e-05\n",
      "Epoch 9700, Loss: 3.39595276215253e-05\n",
      "Epoch 9800, Loss: 3.3441480510775735e-05\n",
      "Epoch 9900, Loss: 3.293829665648357e-05\n",
      "Test Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "X, y = data.data, data.target\n",
    "y = y.reshape(-1, 1) # Ensure y is a column vector\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the neural network\n",
    "layer_sizes = [X_train.shape[1], 64, 32, 1]  # Define the structure of the neural network, 30 input features, 1 output and 2 hidden layers\n",
    "learning_rate = 0.01\n",
    "nn = NeuralNetwork(layer_sizes, lr=learning_rate)\n",
    "nn.train(X_train, y_train, epochs=10000)\n",
    "\n",
    "# Test the trained model on the test set\n",
    "y_pred = nn.forward(X_test)\n",
    "\n",
    "# Convert the predicted values to binary\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(y_pred_binary == y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2bfd1",
   "metadata": {},
   "source": [
    "We have achieved an accuracy of ~0.95 on the test set. Not bad for a simple neural network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
